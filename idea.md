# 方向方法

### 主阶段-1

- 主要就是快速复现别人的代码，尽快刷成绩。对于赛题和数据以及模型没有深刻的理解。排名尚可，但都是别人玩过的了。

### 主阶段-2

##### 计划 & 分析

- 问题：
  - xgboost的0.786的模型是否还有改进空间？
  - 特征数量过少吧？
  - GBDT对于过多的特征貌似影响不大，宁多误少吧？
- 情况：
  - 现在top选手：
    - 第一名开源部分代码，只有特征提取的java代码，而且是2017-10-30最后一次提交
    - 第二名和第十一名交流后使用了模型融合；改进第十一名的代码，苟进前二
    - 第六名下面的使用了多模型训练，提交不知道单模型还是多模型
    - 第十一名开源代码，代码问题多；特征过多106个，模型过多6个；模型融合。直接复现代价太大
    - 根据陈浩阳测试0.81的特征结果：gbdt和xgb分别一个0.7954，一个0.80180
  - 过去情况：
    - top1的auc只有0.78228，且两次的数据和场景基本相似，现在我的auc已经超过这个，所以过去的方案未必有参考价值了
    - 第一名 GBDT给的权重大概是XGBoost的二倍
- 想法：
  - 样本：
    - 线上线下都用
  - 特征：
    - 宁多误少的原则 尝试
  - 模型：
    - GBDT  单模型效果大部分选手说较好
    - XGBoost  加强调参
    - 模型融合 & 模型一定要训练的互补
    - xgboost、lightgbm、catboost的auc和权重不好说；看论文大概就是catboost调好了最好但有局限性，lightgbm和xgboost相近。看错了xgb的auc极限并不是0.79左右
- 步骤：
  1. 从头做起，代码自己手撸（最快大概可能需要5天）
    - 认识数据
    - 数据可视化
    - 数据预处理
    - 特征提取 & 特征要多
    - 尝试多模型，效果预计依次递减：
      - GBDT
      - xgboost
      - lightgbm
      - catboost
      - RandomForestClassifier
      - ExtraTreesClassifier
    - 集成学习：还是就6个模型==》融合
    - 网格搜索

***

### 主阶段-3

- 训练前面的0.78 50名那个，看看score；对比现在
调试gdbt模型
  - 对比结果就是之前的迭代次数特别大

##### 计划 & 分析

- 可以确认的是单模型xgb+116特征或者GDBT+116特征，可以调到一个0.7954，一个0.80180
- 中午12点查看成绩，如果达到上一点的auc证明步骤没有问题；否则查看是否是自己调参或者哪里有问题
- 逆推的概率平均为0.11111111；可以预测后提交的成绩对比
- 查看以前提交的数据的分布图像；分析
- 116个特征还是感觉上过多：
  - 每次删除5%查看auc
  - 逐步删除特征；查看训练的auc
- GDBT模型的预测数据先预测出来
- 然后再使用网格搜索查找GDBT模型的最佳参数
- gdbt和xgb预测结果绘折线走势图对比是否有相干性
  - 之前的所有结果看走势图
- 网格搜索只是遍历自己举例的模型参数，并不能找到实际的参数最优解
  - 自己要调参
- auc到不了的话就是先从特征下手，这个如果gdbt比xgb的auc好的话，有可能会是特征过多的

GDBT训练时间大概不到一小时
中山大学厉害，本科生作业cpp实现xgb。。。
GDBT不支持多线程，慢啊！
